{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vRhaca9Zoe46",
        "outputId": "b710f37a-9ad4-4843-db2a-8f736a86c4d4"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy scipy -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hVwrNray4yk_",
        "outputId": "e3f0a286-91e5-4551-c00f-706eba94a426"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4 scipy==1.13.1\n",
        "# !pip install numpy scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCidiofFQNkR",
        "outputId": "2f0a3faa-32ba-4ebb-d1dd-17b88bdaa920"
      },
      "outputs": [],
      "source": [
        "!pip install smartapi-python\n",
        "!pip install logzero\n",
        "!pip install pyotp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqVV5MP7rbBh",
        "outputId": "543ccb82-77bc-4015-c425-a8b18c4553a0"
      },
      "outputs": [],
      "source": [
        "!pip install smartapi-python --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_v8DByNY96s",
        "outputId": "9728b491-e617-416f-f4c2-dad6fcf30929"
      },
      "outputs": [],
      "source": [
        "!pip install pandas-ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfZjONSZgTc1",
        "outputId": "588dc52a-d75c-4570-c5d6-9baf3d5a38a7"
      },
      "outputs": [],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQABs8eE8U0K"
      },
      "source": [
        "***OOPs Code***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk3q2HgTHHDc"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "import json\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from SmartApi.smartConnect import SmartConnect\n",
        "from pyotp import TOTP\n",
        "from scipy.stats import zscore\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuwlhHPx8Wfm"
      },
      "outputs": [],
      "source": [
        "class AngelOneAPI:\n",
        "  def __init__(self, api_key, user_id, password, totp_key):\n",
        "    self.api_key = api_key\n",
        "    self.user_id = user_id\n",
        "    self.password = password\n",
        "    self.totp_key = totp_key\n",
        "    self.client = SmartConnect(api_key=self.api_key)\n",
        "    self.session_data = None\n",
        "    self.instrument_list = None\n",
        "\n",
        "  def login(self):\n",
        "    totp = TOTP(self.totp_key).now()\n",
        "    self.session_data = self.client.generateSession(self.user_id, self.password, totp)\n",
        "\n",
        "  def fetch_instruments(self, url=\"https://margincalculator.angelbroking.com/OpenAPI_File/files/OpenAPIScripMaster.json\"):\n",
        "    response = urllib.request.urlopen(url)\n",
        "    self.instrument_list = json.loads(response.read())\n",
        "\n",
        "  def token_lookup(self, ticker, exchange=\"NSE\"):\n",
        "    for instrument in self.instrument_list:\n",
        "      if instrument[\"name\"] == ticker and instrument[\"exch_seg\"] == exchange and instrument[\"symbol\"].split('-')[-1] == \"EQ\":\n",
        "          return instrument[\"token\"]\n",
        "    return None\n",
        "\n",
        "  def symbol_lookup(self, token, exchange=\"NSE\"):\n",
        "    for instrument in self.instrument_list:\n",
        "      if instrument[\"token\"] == token and instrument[\"exch_seg\"] == exchange and instrument[\"symbol\"].split('-')[-1] == \"EQ\":\n",
        "          return instrument[\"name\"]\n",
        "    return None\n",
        "\n",
        "  def get_candle_data(self, symbol, from_date, to_date, interval=\"ONE_DAY\", exchange=\"NSE\"):\n",
        "    token = self.token_lookup(symbol, exchange)\n",
        "    if not token:\n",
        "        print(f\"Token not found for {symbol}\")\n",
        "        return None\n",
        "\n",
        "    params = {\n",
        "        \"exchange\": exchange,\n",
        "        \"symboltoken\": str(token),\n",
        "        \"interval\": interval,\n",
        "        \"fromdate\": from_date,\n",
        "        \"todate\": to_date\n",
        "    }\n",
        "\n",
        "    data = self.client.getCandleData(params)\n",
        "    if \"data\" in data and data[\"data\"]:\n",
        "      df = pd.DataFrame(data[\"data\"], columns=[\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"])\n",
        "      df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
        "      df['date'] = df['datetime'].dt.date\n",
        "      df.drop(columns='datetime', inplace=True)\n",
        "      df.set_index('date', inplace=True)\n",
        "      return df\n",
        "    else:\n",
        "      print(f\"No data returned for {symbol}\")\n",
        "      return None\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "  def __init__(self, df):\n",
        "    self.df = df.copy()\n",
        "\n",
        "  def add_volume_indicators(self):\n",
        "    self.df['trade_vol_inr'] = self.df[['close', 'volume']].prod(axis=1).div(1e3)\n",
        "    self.df['z_trade_vol_inr'] = zscore(self.df['trade_vol_inr'])\n",
        "    self.df['z_trade_vol_rank'] = self.df['z_trade_vol_inr'].rolling(window=21).mean().rank(ascending=False)\n",
        "\n",
        "  def add_rsi(self, length=14):\n",
        "    self.df['rsi'] = ta.rsi(self.df['close'], length=length)\n",
        "\n",
        "  def Compute_ADX(self, period = 14):\n",
        "    adx_df = ta.adx(self.df['high'], self.df['low'], self.df['close'], length = period)\n",
        "    if adx_df is not None:\n",
        "      adx_df = adx_df.rename(columns = {f'ADX_{period}':'adx', f'DMP_{period}':'+di', f'DMN_{period}':'-di'})\n",
        "      self.df = pd.concat([self.df, adx_df], axis = 1)\n",
        "    else:\n",
        "      print(f\"ADX could not be computed for period {period}. Not enough data.\")\n",
        "\n",
        "  def Compute_natr_atr(self, period = 14):\n",
        "    natr = ta.natr(self.df['high'], self.df['low'], self.df['close'], length = period)\n",
        "    atr = ta.atr(self.df['high'], self.df['low'], self.df['close'], length=period)\n",
        "    self.df['natr'] = natr\n",
        "    self.df['atr'] = atr\n",
        "\n",
        "  def super_trend(self, period = 14, multiplier = 3):\n",
        "    self.df = self.df.sort_index()\n",
        "    high, low, close = self.df['high'], self.df['low'], self.df['close']\n",
        "    hl2 = (high + low) / 2\n",
        "    atr = ta.atr(high, low, close, length = period)\n",
        "\n",
        "    upper_band = hl2 + (multiplier*atr)\n",
        "    lower_band = hl2 - (multiplier*atr)\n",
        "\n",
        "    supertrend = np.full(len(self.df), np.nan)\n",
        "    trend_dir = np.full(len(self.df), np.nan)\n",
        "\n",
        "    # Check if atr calculation resulted in NaN for the initial period\n",
        "    if not np.isnan(upper_band.iloc[period]):\n",
        "        supertrend[period] = upper_band.iloc[period]\n",
        "        trend_dir[period] = -1\n",
        "    else:\n",
        "        # Handle cases where initial ATR is NaN\n",
        "        supertrend[period] = np.nan\n",
        "        trend_dir[period] = np.nan\n",
        "\n",
        "\n",
        "    for i in range(period + 1, len(self.df)):\n",
        "      if not np.isnan(supertrend[i-1]): # Add this check\n",
        "          if trend_dir[i-1] == 1:\n",
        "            if close.iloc[i] > supertrend[i-1]:\n",
        "              supertrend[i] = max(lower_band.iloc[i], supertrend[i-1])\n",
        "              trend_dir[i] = 1\n",
        "            else:\n",
        "              supertrend[i] = upper_band.iloc[i]\n",
        "              trend_dir[i] = -1\n",
        "          else:\n",
        "            if close.iloc[i] < supertrend[i-1]:\n",
        "              supertrend[i] = min(upper_band.iloc[i], supertrend[i-1])\n",
        "              trend_dir[i] = -1\n",
        "            else:\n",
        "              supertrend[i] = lower_band.iloc[i]\n",
        "              trend_dir[i] = 1\n",
        "      else: # If previous supertrend was NaN, current is also NaN\n",
        "          supertrend[i] = np.nan\n",
        "          trend_dir[i] = np.nan\n",
        "\n",
        "\n",
        "    self.df['supertrend'] = supertrend\n",
        "    self.df['trend_dir'] = trend_dir\n",
        "\n",
        "  def hist_returns(self):\n",
        "    by_sym = self.df['close']\n",
        "    for t in [1, 3, 5, 21]:\n",
        "      col = f'r{t:02}'\n",
        "      self.df[col] = by_sym.pct_change(t)\n",
        "\n",
        "\n",
        "  def fwd_returns(self):\n",
        "    for t in [1, 3, 5, 21]:\n",
        "        col = f'r{t:02}'\n",
        "        fwd_col = f'{col}_fwd'\n",
        "        self.df[fwd_col] = self.df[col].shift(-t)\n",
        "\n",
        "        # Interpolate + extrapolate in-place\n",
        "        self.df[fwd_col] = self.df[fwd_col].interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def features(self):\n",
        "    self.df['daily_return'] = self.df['close'].pct_change()\n",
        "\n",
        "    self.df['returns_ma05'] = self.df['daily_return'].transform(\n",
        "        lambda x: x.shift(1).rolling(5).mean()\n",
        "    )\n",
        "    self.df['volatility_21'] = self.df['daily_return'].transform(\n",
        "        lambda x: x.shift(1).rolling(21).std()\n",
        "    )\n",
        "\n",
        "    self.df['ema_12'] = self.df['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())\n",
        "    self.df['ema_26'] = self.df['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())\n",
        "    self.df['macd'] = self.df['ema_12'] - self.df['ema_26']\n",
        "\n",
        "    self.df['bbw'] = self.df['close'].transform(\n",
        "        lambda x: (x.shift(1).rolling(20).mean() + 2 * x.shift(1).rolling(20).std())\n",
        "                  - (x.shift(1).rolling(20).mean() - 2 * x.shift(1).rolling(20).std())\n",
        "    )\n",
        "\n",
        "    self.df[\"rolling_vol_adj_return\"] = self.df['daily_return'] / self.df[\"atr\"]\n",
        "\n",
        "    self.df['rolling_sharpe_10'] = self.df['daily_return'].transform(\n",
        "        lambda x: x.shift(1).rolling(10).apply(lambda y: np.mean(y) / (np.std(y) + 1e-6))\n",
        "    )\n",
        "\n",
        "    self.df['cum_return'] = self.df['daily_return'].cumsum()\n",
        "\n",
        "    self.df['max_drawdown_21'] = self.df['cum_return'].transform(\n",
        "        lambda x: (x - x.shift(1).rolling(21).max()) / (x.shift(1).rolling(21).max() + 1e-6)\n",
        "    )\n",
        "\n",
        "    self.df['volume_zscore'] = self.df['volume'].transform(\n",
        "        lambda x: (x.shift(1).rolling(20).mean() - x.shift(1).rolling(100).mean()) / (x.shift(1).rolling(100).std())\n",
        "    )\n",
        "\n",
        "    self.df['log_return_1d'] = self.df['close'].transform(lambda x: np.log(x / x.shift(1)))\n",
        "\n",
        "    self.df['atr_percent'] = self.df['atr'] / self.df['close']\n",
        "\n",
        "\n",
        "  def outliers(self):\n",
        "    outliers = self.df[self.df.r01 > 1].index.unique()\n",
        "    self.df = self.df.drop(outliers)\n",
        "\n",
        "  def create_labels(self, long_threshold=0.25, short_threshold=0.25, transaction_cost=0.0003):\n",
        "    returns = self.df['r01_fwd']\n",
        "    q_long = returns.quantile(1 - long_threshold)\n",
        "    q_short = returns.quantile(short_threshold)\n",
        "\n",
        "    self.df['labels'] = 1  # Neutral\n",
        "    self.df.loc[returns > q_long, 'labels'] = 2  # Long\n",
        "    self.df.loc[returns < q_short, 'labels'] = 0\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def remap_labels(series):\n",
        "    return series.map({-1: 0, 0: 1, 1: 2})\n",
        "\n",
        "  def get_result(self):\n",
        "    self.df = self.df.dropna()\n",
        "    # print(self.df)\n",
        "    return self.df\n",
        "\n",
        "\n",
        "\n",
        "#class for Classification\n",
        "class LGBTimeSeriesRegressor():\n",
        "  def __init__(self, df):\n",
        "    self.df = df.copy()\n",
        "\n",
        "  def features_split(self):\n",
        "    features = [col for col in self.df.columns if col not in ['r01_fwd', 'r05_fwd', 'r21_fwd', 'labels','open', 'close', 'low', 'high', 'volume']]\n",
        "    self.X = self.df[features]\n",
        "    self.y = self.df['labels']\n",
        "    return self.X, self.y\n",
        "\n",
        "  def train_test_split(self, train_years=7, val_years=1, test_years=1):\n",
        "    # print(self.X)\n",
        "    last_date = self.X.index.max()\n",
        "\n",
        "    test_start = last_date - pd.DateOffset(years=test_years) + pd.DateOffset(days=1)\n",
        "    val_start = test_start - pd.DateOffset(years=val_years)\n",
        "    train_start = val_start - pd.DateOffset(years=train_years)\n",
        "\n",
        "    train_mask = (self.X.index >= train_start) & (self.X.index < val_start)\n",
        "    val_mask = (self.X.index >= val_start) & (self.X.index < test_start)\n",
        "    test_mask = (self.X.index >= test_start)\n",
        "\n",
        "    self.X_train, self.y_train = self.X[train_mask], self.y[train_mask]\n",
        "    self.X_val, self.y_val = self.X[val_mask], self.y[val_mask]\n",
        "    self.X_test, self.y_test = self.X[test_mask], self.y[test_mask]\n",
        "\n",
        "\n",
        "    # print(self.X_test)\n",
        "    return self.X_train, self.X_val, self.X_test, self.y_test\n",
        "\n",
        "\n",
        "\n",
        "  def calc_weights(self, y):\n",
        "    class_counts = y.value_counts()\n",
        "    total = sum(class_counts)\n",
        "\n",
        "    return y.map({cls: total / (len(class_counts) * count) for cls, count in class_counts.items()})\n",
        "\n",
        "\n",
        "  def f1_custom(self, preds, data):\n",
        "    labels = data.get_label()\n",
        "    preds = preds.argmax(axis=1)\n",
        "    return 'f1_macro', f1_score(labels, preds, average='macro'), True\n",
        "\n",
        "  def objective(self, trial):\n",
        "    params = {\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 3,\n",
        "        'metric': 'custom',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.002, 0.1, log = True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'min_child_samples':trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
        "        'min_split_gain': trial.suggest_float('min_split_gain', 0.01, 0.1),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 250, 300),\n",
        "        'random_state':42,\n",
        "        'n_jobs': -1,\n",
        "        'verbosity':-1,\n",
        "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
        "        'feature_fraction_bynode': trial.suggest_float('feature_fraction_bynode', 0.5, 1.0),\n",
        "    }\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(tscv.split(self.X_train)):\n",
        "      train_idx = train_idx[train_idx <= val_idx[0] - 30]  # purge lookahead leak\n",
        "\n",
        "      # Drop NaNs from train and validation subsets\n",
        "      X_train_fold = self.X_train.iloc[train_idx].dropna()\n",
        "      y_train_fold = self.y_train.iloc[train_idx].dropna()\n",
        "      X_val_fold = self.X_train.iloc[val_idx].dropna()\n",
        "      y_val_fold = self.y_train.iloc[val_idx].dropna()\n",
        "\n",
        "\n",
        "      weights = self.calc_weights(y_train_fold)\n",
        "      dtrain = lgb.Dataset(X_train_fold, y_train_fold, weight=weights)\n",
        "      dval = lgb.Dataset(X_val_fold, y_val_fold, reference=dtrain)\n",
        "\n",
        "      model = lgb.train(params,\n",
        "                        dtrain,\n",
        "                        valid_sets=[dtrain, dval],\n",
        "                        feval=self.f1_custom,\n",
        "                        callbacks=[\n",
        "                            lgb.early_stopping(50, verbose=False),\n",
        "                            lgb.log_evaluation(0)\n",
        "                        ])\n",
        "\n",
        "      preds = model.predict(X_val_fold).argmax(axis=1)\n",
        "      score = f1_score(y_val_fold, preds, average='macro')\n",
        "      scores.append(score)\n",
        "\n",
        "    return np.mean(scores)\n",
        "\n",
        "  def run_optuna(self, n_trials=20):\n",
        "    self.study = optuna.create_study(direction='maximize')\n",
        "    self.study.optimize(self.objective, n_trials=n_trials, show_progress_bar=False, catch=(Exception,))\n",
        "    return self.study.best_params\n",
        "\n",
        "  def train_final_model(self):\n",
        "    best_params.update({\n",
        "        'objective': 'multiclass',\n",
        "        'num_class': 3,\n",
        "        'metric': 'custom',\n",
        "        'verbosity': -1\n",
        "    })\n",
        "\n",
        "    train_data = lgb.Dataset(self.X_train, self.y_train, weight=self.calc_weights(self.y_train))\n",
        "    val_data = lgb.Dataset(self.X_val, self.y_val, reference=train_data)\n",
        "\n",
        "    self.final_model = lgb.train(best_params,\n",
        "                                  train_data,\n",
        "                                  valid_sets=[train_data, val_data],\n",
        "                                  feval=self.f1_custom,\n",
        "                                  num_boost_round=1000,\n",
        "                                  callbacks=[\n",
        "                                      lgb.early_stopping(50, verbose = False),\n",
        "                                      lgb.log_evaluation(0)\n",
        "                                  ])\n",
        "    return self.final_model\n",
        "\n",
        "  # def adjust_predictions(self, pred_prob, short_thresh = 0.65, long_thresh = 0.60):\n",
        "  #   adjusted_preds = []\n",
        "  #   for prob in pred_prob:\n",
        "  #     if prob[0] > short_thresh:\n",
        "  #       adjusted_preds.append(0)\n",
        "  #     elif prob[2] > long_thresh:\n",
        "  #       adjusted_preds.append(2)\n",
        "  #     else:\n",
        "  #       adjusted_preds.append(1)\n",
        "  #   return np.array(adjusted_preds)\n",
        "\n",
        "  def adjust_predictions(self, pred_prob):\n",
        "    return np.argmax(pred_prob, axis=1)\n",
        "\n",
        "\n",
        "  def validation(self):\n",
        "    val_probs = self.final_model.predict(self.X_val)\n",
        "    val_preds = self.adjust_predictions(val_probs)\n",
        "    # print(classification_report(self.y_val, val_preds, target_names=['Short', 'Neutral', 'Long']))\n",
        "    # print(confusion_matrix(self.y_val, val_preds))\n",
        "\n",
        "    return val_probs\n",
        "\n",
        "  def training(self):\n",
        "    train_probs = self.final_model.predict(self.X_train)\n",
        "    train_preds = self.adjust_predictions(train_probs)\n",
        "    # print(classification_report(self.y_train, train_preds, target_names=['Short', 'Neutral', 'Long']))\n",
        "    # print(confusion_matrix(self.y_train, train_preds))\n",
        "\n",
        "    return train_probs\n",
        "\n",
        "class Strategy():\n",
        "  def __init__(self, df, val_probs, train_probs, X_train, X_val, X_test, y_test, final_model, regressor, transaction_cost):\n",
        "    self.df = df.copy()\n",
        "    self.val_probs = val_probs\n",
        "    self.train_probs = train_probs\n",
        "    self.X_train = X_train\n",
        "    self.X_val = X_val\n",
        "    self.X_test = X_test\n",
        "    self.y_test = y_test\n",
        "    self.transaction_cost = transaction_cost\n",
        "    self.final_model = final_model\n",
        "    self.regressor = regressor\n",
        "\n",
        "  def simulate_strategy(self, pred_probs, true_returns, cost_per_trade):\n",
        "    volatility = true_returns.rolling(window = 20).std()\n",
        "    cost_per_trade *= volatility.mean()\n",
        "\n",
        "    short_prob, neutral_prob, long_prob = pred_probs.T\n",
        "    positions = np.clip(np.where(long_prob > 0.4, 1.2, np.where(short_prob > 0.35, -1.2, long_prob-short_prob)), -1.5, 1.5)\n",
        "\n",
        "    trades = np.abs(np.diff(positions, prepend=0))\n",
        "    Strategy_returns = positions*true_returns - trades*cost_per_trade\n",
        "\n",
        "    return Strategy_returns, Strategy_returns.mean()/Strategy_returns.std(), (Strategy_returns > 0).mean()\n",
        "\n",
        "  def valid_simulate(self):\n",
        "    val_returns = self.df.loc[self.X_val.index, 'r01_fwd']\n",
        "    val_strategy_returns, val_sharpe, val_win_rate = self.simulate_strategy(self.val_probs, val_returns, self.transaction_cost)\n",
        "\n",
        "    # print(f\"\\nValidation Strategy Performance:\")\n",
        "    # print(f\"Sharpe Ratio: {val_sharpe:.4f}\")\n",
        "    # print(f\"Win Rate: {val_win_rate:.2%}\")\n",
        "\n",
        "  def training_simulate(self):\n",
        "    train_returns = self.df.loc[self.X_train.index, 'r01_fwd']\n",
        "    train_strategy_returns, train_sharpe, train_win_rate = self.simulate_strategy(self.train_probs, train_returns, self.transaction_cost)\n",
        "\n",
        "    # print(f\"\\nTraining Strategy Performance:\")\n",
        "    # print(f\"Sharpe Ratio: {train_sharpe:.4f}\")\n",
        "    # print(f\"Win Rate: {train_win_rate:.2%}\")\n",
        "\n",
        "  def testing(self):\n",
        "    test_probs = self.final_model.predict(self.X_test)\n",
        "    test_preds = self.regressor.adjust_predictions(test_probs)\n",
        "\n",
        "    # print(\"Classification report\")\n",
        "    # print(classification_report(self.y_test, test_preds, target_names=['Short', 'Neutral', 'Long']))\n",
        "\n",
        "\n",
        "    test_returns = self.df.loc[self.X_test.index, 'r01_fwd']\n",
        "    test_strategy_returns, test_sharpe, test_win_rate = self.simulate_strategy(test_probs, test_returns, self.transaction_cost)\n",
        "\n",
        "    # print(f\"\\nTesting Strategy Performance:\")\n",
        "    # print(f\"Sharpe Ratio: {test_sharpe:.4f}\")\n",
        "    # print(f\"Win Rate: {test_win_rate:.2%}\")\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "\n",
        "\n",
        "#Class for regression\n",
        "class LGBTimeSeriesRegressor_Regression():\n",
        "  def __init__(self, df, target_col='r01_fwd', cost_per_trade=0.0003):\n",
        "    self.df = df.copy()\n",
        "    self.target_col = target_col\n",
        "    self.cost_per_trade = cost_per_trade\n",
        "    self.features = []\n",
        "    self.final_model = None\n",
        "    self.study = None\n",
        "    self.best_params = None\n",
        "    self.test_results = None\n",
        "\n",
        "  def features_split(self):\n",
        "    exclude = ['open', 'close', 'low', 'high', 'volume', 'labels',\n",
        "                'r05_fwd', 'r21_fwd'] # Removed 'r01_fwd' from exclude list\n",
        "    self.features = [col for col in self.df.columns if col not in exclude]\n",
        "    self.X = self.df[self.features]\n",
        "    self.y = self.df[self.target_col]\n",
        "    return self.X, self.y\n",
        "\n",
        "  def train_test_split(self, train_years=7, val_years=1, test_years=1):\n",
        "    date_index = self.X.index  # Already datetime64[ns]\n",
        "    # print(self.X)\n",
        "    last_date = date_index.max()\n",
        "\n",
        "    # Compute dynamic rolling window boundaries\n",
        "    test_start = last_date - pd.DateOffset(years=test_years) + pd.DateOffset(days=1)\n",
        "    val_start = test_start - pd.DateOffset(years=val_years)\n",
        "    train_start = val_start - pd.DateOffset(years=train_years)\n",
        "\n",
        "    # Boolean masks\n",
        "    train_mask = (date_index >= train_start) & (date_index < val_start)\n",
        "    val_mask = (date_index >= val_start) & (date_index < test_start)\n",
        "    test_mask = (date_index >= test_start)\n",
        "\n",
        "    # Split X and y\n",
        "    self.X_train, self.y_train = self.X[train_mask], self.y[train_mask]\n",
        "    self.X_val, self.y_val = self.X[val_mask], self.y[val_mask]\n",
        "    self.X_test, self.y_test = self.X[test_mask], self.y[test_mask]\n",
        "\n",
        "    # Replace infs with NaNs\n",
        "    self.X_train = self.X_train.replace([np.inf, -np.inf], np.nan)\n",
        "    self.X_val = self.X_val.replace([np.inf, -np.inf], np.nan)\n",
        "    self.X_test = self.X_test.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Drop rows where y_train is NaN\n",
        "    mask = ~self.y_train.isna()\n",
        "    self.X_train, self.y_train = self.X_train[mask], self.y_train[mask]\n",
        "\n",
        "    # Impute missing values (fit only on train, transform others)\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    self.X_train = pd.DataFrame(imputer.fit_transform(self.X_train), index=self.X_train.index, columns=self.X_train.columns)\n",
        "    self.X_val = pd.DataFrame(imputer.transform(self.X_val), index=self.X_val.index, columns=self.X_val.columns)\n",
        "    self.X_test = pd.DataFrame(imputer.transform(self.X_test), index=self.X_test.index, columns=self.X_test.columns)\n",
        "\n",
        "\n",
        "    # print(self.X_test)\n",
        "    return self.X_train, self.X_val, self.X_test, self.y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def rmse(self, y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "  def compute_sharpe_ratio(self, y_true, y_pred):\n",
        "    returns = y_true * np.sign(y_pred)\n",
        "    return returns.mean() / returns.std() if returns.std() != 0 else 0\n",
        "\n",
        "  def compute_strategy_win_rate(self, y_true, y_pred):\n",
        "    signal = np.sign(y_pred)\n",
        "    strategy_returns = y_true * signal - self.cost_per_trade\n",
        "    return (strategy_returns > 0).mean()\n",
        "\n",
        "  def compute_accuracy(self, y_true, y_pred):\n",
        "    return np.mean(np.sign(y_true) == np.sign(y_pred))\n",
        "\n",
        "  def objective(self, trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'rmse',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.01, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),\n",
        "        'random_state': 42,\n",
        "        'verbosity': -1,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    rmses = []\n",
        "\n",
        "    for train_idx, val_idx in tscv.split(self.X_train):\n",
        "        lgb_train = lgb.Dataset(self.X_train.iloc[train_idx], self.y_train.iloc[train_idx])\n",
        "        lgb_val = lgb.Dataset(self.X_train.iloc[val_idx], self.y_train.iloc[val_idx])\n",
        "\n",
        "        model = lgb.train(params, lgb_train, valid_sets=[lgb_val],\n",
        "                          callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)])\n",
        "        preds = model.predict(self.X_train.iloc[val_idx])\n",
        "        rmses.append(self.rmse(self.y_train.iloc[val_idx], preds))\n",
        "\n",
        "    return np.mean(rmses)\n",
        "\n",
        "  def run_optuna(self, n_trials=15):\n",
        "    self.study = optuna.create_study(direction='minimize')\n",
        "    self.study.optimize(self.objective, n_trials=n_trials, show_progress_bar=False)\n",
        "    self.best_params = self.study.best_trial.params\n",
        "    return self.best_params\n",
        "\n",
        "  def train_final_model(self, use_val=True):\n",
        "    params = {\n",
        "            **self.best_params,\n",
        "            'objective': 'regression',\n",
        "            'metric': 'rmse',\n",
        "            'verbosity': -1\n",
        "        }\n",
        "\n",
        "    if use_val:\n",
        "        train_data = lgb.Dataset(self.X_train, self.y_train)\n",
        "        val_data = lgb.Dataset(self.X_val, self.y_val, reference=train_data)\n",
        "    else:\n",
        "        X_trainval = pd.concat([self.X_train, self.X_val])\n",
        "        y_trainval = pd.concat([self.y_train, self.y_val])\n",
        "        train_data = lgb.Dataset(X_trainval, y_trainval)\n",
        "        val_data = train_data  # dummy\n",
        "\n",
        "    self.final_model = lgb.train(params, train_data, valid_sets=[val_data],\n",
        "                                  callbacks=[lgb.early_stopping(100, verbose=False), lgb.log_evaluation(0)])\n",
        "    return self.final_model\n",
        "\n",
        "  def evaluate_model(self, X, y, dataset_name=\"\"):\n",
        "    preds = self.final_model.predict(X)\n",
        "    r2 = r2_score(y, preds)\n",
        "    rmse_val = self.rmse(y, preds)\n",
        "    sharpe = self.compute_sharpe_ratio(y, preds)\n",
        "    win_rate = self.compute_strategy_win_rate(y, preds)\n",
        "    # accuracy = self.compute_accuracy(y, preds)\n",
        "\n",
        "    # print(f\"\\n{dataset_name} Performance:\")\n",
        "    # print(f\"RMSE: {rmse_val:.6f}\")\n",
        "    # print(f\"RÂ² Score: {r2:.6f}\")\n",
        "    # print(f\"Sharpe Ratio: {sharpe:.6f}\")\n",
        "    # print(f\"Win Rate: {win_rate * 100:.2f}%\")\n",
        "    # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    return preds\n",
        "\n",
        "  def generate_strategy_results(self, y_preds, y_true, X):\n",
        "    y_true = y_true.loc[X.index]\n",
        "    X = X.loc[y_true.index]\n",
        "    self.test_results = pd.DataFrame({\n",
        "        'Predicted_Return': y_preds,\n",
        "        'Actual_Return': y_true\n",
        "    }, index=X.index)\n",
        "    self.test_results['Position'] = np.sign(self.test_results['Predicted_Return'])\n",
        "    self.test_results['Strategy_Return'] = self.test_results['Actual_Return'] * self.test_results['Position']\n",
        "    return self.test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "collapsed": true,
        "id": "ByH1vX5x8aWb",
        "outputId": "ee3e245f-c458-4586-e9e1-814a8843f23c"
      },
      "outputs": [],
      "source": [
        "api_key = \"\"\n",
        "user_id = \"\"\n",
        "password = \"\"\n",
        "key = \"\"\n",
        "\n",
        "angel = AngelOneAPI(api_key, user_id, password, key)\n",
        "angel.login()\n",
        "angel.fetch_instruments()\n",
        "\n",
        "stocks_with_long_signals = []\n",
        "stocks_with_short_signals = []\n",
        "\n",
        "ticker_list = [\n",
        "    'ABB', 'ADANIENT', 'ADANIPORTS', 'ADANIPOWER', 'AMBUJACEM', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK', 'BAJAJ-AUTO',\n",
        "    'BAJFINANCE', 'BAJAJFINSV', 'BAJAJHLDNG', 'BAJAJHFL', 'BANKBARODA', 'BEL', 'BPCL', 'BHARTIARTL', 'BOSCHLTD', 'BRITANNIA',\n",
        "    'CGPOWER', 'CANBK', 'CHOLAFIN', 'CIPLA', 'COALINDIA', 'COALINDIA', 'DLF', 'DABUR', 'DIVISLAB', 'DRREDDY', 'EICHERMOT', 'ETERNAL',\n",
        "    'GAIL', 'GODREJCP', 'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE', 'HAVELLS', 'HEROMOTOCO', 'HINDALCO', 'HAL', 'HINDUNILVR', 'HYUNDAI', 'ICICIBANK',\n",
        "    'ICICIGI', 'ICICIPRULI', 'ITC', 'INDHOTEL', 'IOC', 'IRFC', 'INDUSINDBK', 'NAUKRI', 'INFY', 'INDIGO', 'JSWENERGY', 'JSWSTEEL',\n",
        "    'JINDALSTEL', 'KOTAKBANK', 'LTIM','LT', 'LICI', 'M&M', 'MARUTI', 'NTPC', 'NESTLEIND', 'ONGC', 'PIDILITIND',\n",
        "    'PFC', 'POWERGRID', 'PNB', 'RECLTD', 'RELIANCE', 'SBILIFE', 'MOTHERSON', 'SHREECEM', 'SHRIRAMFIN', 'SIEMENS', 'SBIN', 'SUNPHARMA', 'TVSMOTOR'\n",
        "    'TCS', 'TATACONSUM', 'TATAMOTORS', 'TATAPOWER', 'TATASTEEL', 'TECHM', 'TITAN', 'TORNTPHARM', 'TRENT', 'ULTRACEMCO', 'UNITDSPR', 'VBL', 'VEDL', 'WIPRO', 'ZYDUSLIFE'\n",
        "]\n",
        "df_dict = {}\n",
        "results = {}\n",
        "\n",
        "for ticker in ticker_list:\n",
        "  end_time = datetime.now().replace(hour=15, minute=30, second=0, microsecond=0)\n",
        "  mid_time = end_time - relativedelta(years=5)\n",
        "\n",
        "  start_time = end_time - relativedelta(years=10)\n",
        "  start_time = start_time.replace(hour=9, minute=15)\n",
        "  mid_time = mid_time.replace(hour=15, minute=30)\n",
        "  end_start = mid_time.replace(hour=9, minute=15)\n",
        "\n",
        "  start_str = start_time.strftime(\"%Y-%m-%d %H:%M\")\n",
        "  mid_str = mid_time.strftime(\"%Y-%m-%d %H:%M\")\n",
        "  end_start_str = end_start.strftime(\"%Y-%m-%d %H:%M\")\n",
        "  end_str = end_time.strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "\n",
        "  df1 = angel.get_candle_data(ticker, start_str, mid_str)\n",
        "  df2 = angel.get_candle_data(ticker, end_start_str, end_str)\n",
        "\n",
        "  df1.index = pd.to_datetime(df1.index, errors='coerce')\n",
        "  df2.index = pd.to_datetime(df2.index, errors='coerce')\n",
        "\n",
        "  df = pd.concat([df1, df2])\n",
        "  # print(df)\n",
        "\n",
        "  if df is not None:\n",
        "      analyzer = Dataset(df)\n",
        "      analyzer.add_volume_indicators()\n",
        "      analyzer.add_rsi()\n",
        "      analyzer.Compute_ADX()\n",
        "      analyzer.Compute_natr_atr()\n",
        "      analyzer.super_trend()\n",
        "      analyzer.hist_returns()\n",
        "      analyzer.fwd_returns()\n",
        "      analyzer.features()\n",
        "      analyzer.outliers()\n",
        "      analyzer.create_labels()\n",
        "      # analyzer.remap_labels()\n",
        "      df_dict[ticker] = analyzer.get_result()\n",
        "      # print(df_dict[ticker])\n",
        "      # print(df_dict[ticker].columns.tolist())\n",
        "\n",
        "      LGB = LGBTimeSeriesRegressor(df_dict[ticker])\n",
        "      X, y = LGB.features_split()\n",
        "      X_train, X_val, X_test, y_test = LGB.train_test_split()\n",
        "      best_params = LGB.run_optuna()\n",
        "      final_model = LGB.train_final_model()\n",
        "      # print(f\"Result for {ticker}: \")\n",
        "      val_prob = LGB.validation()\n",
        "      train_prob = LGB.training()\n",
        "\n",
        "      # print(X_test)\n",
        "\n",
        "      # Pass the 'r01_fwd' for the test set index to the Strategy class\n",
        "      test_returns_for_strategy = df_dict[ticker].loc[X_test.index, 'r01_fwd']\n",
        "      strategy = Strategy(df_dict[ticker], val_prob, train_prob, X_train, X_val, X_test, y_test, final_model, LGB, 0.0003)\n",
        "      strategy.valid_simulate()\n",
        "      strategy.training_simulate()\n",
        "      y_preds_classification = strategy.testing()\n",
        "\n",
        "\n",
        "\n",
        "      reg = LGBTimeSeriesRegressor_Regression(df_dict[ticker], target_col='r01_fwd')\n",
        "      X, y = reg.features_split()\n",
        "      reg.train_test_split()\n",
        "      reg.run_optuna(n_trials=20)\n",
        "      reg.train_final_model(use_val=True)\n",
        "      reg.evaluate_model(reg.X_train, reg.y_train, \"Train\")\n",
        "      reg.evaluate_model(reg.X_val, reg.y_val, \"Validation\")\n",
        "      reg.evaluate_model(reg.X_test, reg.y_test.dropna(), \"Test\")\n",
        "      reg.train_final_model(use_val=False)\n",
        "      y_test_preds = reg.evaluate_model(reg.X_test, reg.y_test.dropna(), \"Test (retrained)\")\n",
        "      strategy_df = reg.generate_strategy_results(y_test_preds, reg.y_test, reg.X_test)\n",
        "      y_preds_regression = strategy_df['Position']\n",
        "      # print(strategy_df.tail(10))\n",
        "\n",
        "      y_pred_classifcation = pd.Series(y_preds_classification, index=y_preds_regression.index)\n",
        "\n",
        "      combined = pd.DataFrame({\n",
        "          'reg': y_preds_regression,\n",
        "          'class': y_pred_classifcation\n",
        "      })\n",
        "\n",
        "      agreement = np.where(\n",
        "            (y_preds_regression == 1) & (y_pred_classifcation == 2), 1,\n",
        "            np.where((y_preds_regression == -1) & (y_pred_classifcation == 0), -1, 0)\n",
        "        )\n",
        "\n",
        "      print()\n",
        "      print(f\"The result for {ticker}\")\n",
        "      print(agreement[-1])\n",
        "      if agreement[-1] == -1:\n",
        "        stocks_with_short_signals.append(ticker)\n",
        "      elif agreement[-1] == 1:\n",
        "        stocks_with_long_signals.append(ticker)\n",
        "      print()\n",
        "      print()\n",
        "\n",
        "print(stocks_with_short_signals)\n",
        "print()\n",
        "print(stocks_with_long_signals)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
